{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12727d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Marathi data...\n",
      "Loaded 1000 Marathi text samples\n",
      "Script: Devanagari\n",
      "Sample 1: ऊती संवर्धन तंत्राचे अनेक उपयोग आहेत. या तंत्राचा उपयोग विशेषकरून जीवशास्त्र व वैद्यकशास्त्रात होतो....\n",
      "Sample 2: ...\n",
      "Sample 3: शहरातील माध्यमिक विभागाच्या शाळा ३ जानेवारीपर्यंत विद्यार्थ्यांसाठी बंद ठेवण्यात येणार आहेत. मात्र, ...\n",
      "Loaded 1000 Marathi text samples\n",
      "Script: Devanagari\n",
      "Sample 1: ऊती संवर्धन तंत्राचे अनेक उपयोग आहेत. या तंत्राचा उपयोग विशेषकरून जीवशास्त्र व वैद्यकशास्त्रात होतो....\n",
      "Sample 2: ...\n",
      "Sample 3: शहरातील माध्यमिक विभागाच्या शाळा ३ जानेवारीपर्यंत विद्यार्थ्यांसाठी बंद ठेवण्यात येणार आहेत. मात्र, ...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Configuration for different Indian languages supported by IndicCorpV2\n",
    "LANGUAGE_CONFIG = {\n",
    "    # Hindi\n",
    "    \"hin_Deva\": {\"name\": \"Hindi\", \"script\": \"Devanagari\", \"unicode_range\": \"\\\\u0900-\\\\u097F\"},\n",
    "    # Marathi  \n",
    "    \"mar_Deva\": {\"name\": \"Marathi\", \"script\": \"Devanagari\", \"unicode_range\": \"\\\\u0900-\\\\u097F\"},\n",
    "    # Gujarati\n",
    "    \"guj_Gujr\": {\"name\": \"Gujarati\", \"script\": \"Gujarati\", \"unicode_range\": \"\\\\u0A80-\\\\u0AFF\"},\n",
    "    # Bengali\n",
    "    \"ben_Beng\": {\"name\": \"Bengali\", \"script\": \"Bengali\", \"unicode_range\": \"\\\\u0980-\\\\u09FF\"},\n",
    "    # Tamil\n",
    "    \"tam_Taml\": {\"name\": \"Tamil\", \"script\": \"Tamil\", \"unicode_range\": \"\\\\u0B80-\\\\u0BFF\"},\n",
    "    # Telugu\n",
    "    \"tel_Telu\": {\"name\": \"Telugu\", \"script\": \"Telugu\", \"unicode_range\": \"\\\\u0C00-\\\\u0C7F\"},\n",
    "    # Kannada\n",
    "    \"kan_Knda\": {\"name\": \"Kannada\", \"script\": \"Kannada\", \"unicode_range\": \"\\\\u0C80-\\\\u0CFF\"},\n",
    "    # Malayalam\n",
    "    \"mal_Mlym\": {\"name\": \"Malayalam\", \"script\": \"Malayalam\", \"unicode_range\": \"\\\\u0D00-\\\\u0D7F\"},\n",
    "    # Punjabi\n",
    "    \"pan_Guru\": {\"name\": \"Punjabi\", \"script\": \"Gurmukhi\", \"unicode_range\": \"\\\\u0A00-\\\\u0A7F\"},\n",
    "    # Oriya/Odia\n",
    "    \"ori_Orya\": {\"name\": \"Odia\", \"script\": \"Odia\", \"unicode_range\": \"\\\\u0B00-\\\\u0B7F\"},\n",
    "    # Assamese\n",
    "    \"asm_Beng\": {\"name\": \"Assamese\", \"script\": \"Bengali\", \"unicode_range\": \"\\\\u0980-\\\\u09FF\"},\n",
    "    # Urdu\n",
    "    \"urd_Arab\": {\"name\": \"Urdu\", \"script\": \"Arabic\", \"unicode_range\": \"\\\\u0600-\\\\u06FF\"},\n",
    "}\n",
    "\n",
    "# Configure the language you want to process\n",
    "SELECTED_LANGUAGE = \"mar_Deva\"  # Change this to any supported language\n",
    "\n",
    "# Load data from IndicCorpV2 dataset\n",
    "print(f\"Loading {LANGUAGE_CONFIG[SELECTED_LANGUAGE]['name']} data...\")\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=SELECTED_LANGUAGE, streaming=True)\n",
    "\n",
    "# Convert streaming dataset to list of texts (first 1000 samples)\n",
    "texts = []\n",
    "count = 0\n",
    "for sample in dataset:\n",
    "    texts.append(sample['text'])\n",
    "    count += 1\n",
    "    if count >= 1000:  # Limit to avoid memory issues\n",
    "        break\n",
    "\n",
    "# Preview a few paragraphs\n",
    "print(f\"Loaded {len(texts)} {LANGUAGE_CONFIG[SELECTED_LANGUAGE]['name']} text samples\")\n",
    "print(f\"Script: {LANGUAGE_CONFIG[SELECTED_LANGUAGE]['script']}\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    print(f\"Sample {i+1}: {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cac86048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Marathi data from split: mar_Deva\n",
      "Script: Devanagari\n",
      "Successfully loaded 1000 text samples\n",
      "Sample 1: ऊती संवर्धन तंत्राचे अनेक उपयोग आहेत. या तंत्राचा उपयोग विशेषकरून जीवशास्त्र व वैद्यकशास्त्रात होतो....\n",
      "Sample 2: ...\n",
      "Sample 3: शहरातील माध्यमिक विभागाच्या शाळा ३ जानेवारीपर्यंत विद्यार्थ्यांसाठी बंद ठेवण्यात येणार आहेत. मात्र, ...\n",
      "Successfully loaded 1000 text samples\n",
      "Sample 1: ऊती संवर्धन तंत्राचे अनेक उपयोग आहेत. या तंत्राचा उपयोग विशेषकरून जीवशास्त्र व वैद्यकशास्त्रात होतो....\n",
      "Sample 2: ...\n",
      "Sample 3: शहरातील माध्यमिक विभागाच्या शाळा ३ जानेवारीपर्यंत विद्यार्थ्यांसाठी बंद ठेवण्यात येणार आहेत. मात्र, ...\n"
     ]
    }
   ],
   "source": [
    "def detect_language_from_split(split_name):\n",
    "    \"\"\"\n",
    "    Automatically detect language configuration based on split name\n",
    "    \"\"\"\n",
    "    if split_name in LANGUAGE_CONFIG:\n",
    "        return LANGUAGE_CONFIG[split_name]\n",
    "    else:\n",
    "        # If split not in config, try to infer basic info\n",
    "        parts = split_name.split('_')\n",
    "        if len(parts) == 2:\n",
    "            lang_code, script_code = parts\n",
    "            return {\n",
    "                \"name\": lang_code.upper(),\n",
    "                \"script\": script_code,\n",
    "                \"unicode_range\": \"\\\\u0000-\\\\uFFFF\"  # Default to full unicode\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"name\": \"Unknown\",\n",
    "                \"script\": \"Unknown\", \n",
    "                \"unicode_range\": \"\\\\u0000-\\\\uFFFF\"\n",
    "            }\n",
    "\n",
    "def load_language_data(language_split, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Load data for any language split from IndicCorpV2\n",
    "    \"\"\"\n",
    "    # Get language info\n",
    "    lang_info = detect_language_from_split(language_split)\n",
    "    \n",
    "    print(f\"Loading {lang_info['name']} data from split: {language_split}\")\n",
    "    print(f\"Script: {lang_info['script']}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"ai4bharat/IndicCorpV2\", \"indiccorp_v2\", split=language_split, streaming=True)\n",
    "        \n",
    "        # Convert streaming dataset to list of texts\n",
    "        texts = []\n",
    "        count = 0\n",
    "        for sample in dataset:\n",
    "            texts.append(sample['text'])\n",
    "            count += 1\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Successfully loaded {len(texts)} text samples\")\n",
    "        \n",
    "        # Preview samples\n",
    "        for i, text in enumerate(texts[:3]):\n",
    "            print(f\"Sample {i+1}: {text[:100]}...\")\n",
    "            \n",
    "        return texts, lang_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {language_split}: {e}\")\n",
    "        print(\"Available splits might include:\")\n",
    "        for split in LANGUAGE_CONFIG.keys():\n",
    "            print(f\"  - {split} ({LANGUAGE_CONFIG[split]['name']})\")\n",
    "        return [], lang_info\n",
    "\n",
    "# Example usage - change this split to any language you want\n",
    "TARGET_LANGUAGE_SPLIT = \"mar_Deva\"  # Change this to test different languages\n",
    "texts, current_lang_info = load_language_data(TARGET_LANGUAGE_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61cca316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created tokenizer for Marathi (Devanagari script)\n",
      "Unicode range: \\u0900-\\u097F\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sentence tokenizer remains the same for all languages\n",
    "def sentence_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Universal sentence tokenizer that works for most languages\n",
    "    \"\"\"\n",
    "    # Use multiple sentence ending patterns for different scripts\n",
    "    sentence_patterns = [\n",
    "        r'(?<=[.!?])\\s+',     # English punctuation\n",
    "        r'(?<=[।॥])\\s+',      # Devanagari punctuation (Hindi, Marathi, etc.)\n",
    "        r'(?<=[؟۔])\\s+',       # Arabic/Urdu punctuation\n",
    "        r'(?<=[។៕])\\s+',      # Khmer punctuation\n",
    "        r'(?<=[။၊])\\s+',       # Myanmar punctuation\n",
    "    ]\n",
    "    \n",
    "    # Combine all patterns\n",
    "    combined_pattern = '|'.join(sentence_patterns)\n",
    "    return re.split(combined_pattern, text.strip())\n",
    "\n",
    "def create_language_aware_word_tokenizer(lang_info):\n",
    "    \"\"\"\n",
    "    Create a word tokenizer optimized for the specific language\n",
    "    \"\"\"\n",
    "    def word_tokenizer(sentence):\n",
    "        # Common patterns for all languages\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        email_pattern = r'\\S+@\\S+\\.\\S+'\n",
    "        date_pattern = r'\\b\\d{1,2}[-/\\.]\\d{1,2}[-/\\.]\\d{2,4}\\b'\n",
    "        number_pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "        \n",
    "        # Language-specific punctuation patterns\n",
    "        if lang_info['script'] == 'Devanagari':\n",
    "            punctuation_pattern = r'[।॥!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "        elif lang_info['script'] == 'Arabic':\n",
    "            punctuation_pattern = r'[؟۔!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "        elif lang_info['script'] == 'Bengali':\n",
    "            punctuation_pattern = r'[।॥!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "        else:\n",
    "            punctuation_pattern = r'[।॥!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "        \n",
    "        # Comprehensive Unicode word patterns for all major scripts\n",
    "        unicode_word_patterns = [\n",
    "            r'[\\u0900-\\u097F]+',  # Devanagari (Hindi, Marathi, Sanskrit, etc.)\n",
    "            r'[\\u0980-\\u09FF]+',  # Bengali\n",
    "            r'[\\u0A00-\\u0A7F]+',  # Gurmukhi (Punjabi)\n",
    "            r'[\\u0A80-\\u0AFF]+',  # Gujarati\n",
    "            r'[\\u0B00-\\u0B7F]+',  # Oriya\n",
    "            r'[\\u0B80-\\u0BFF]+',  # Tamil\n",
    "            r'[\\u0C00-\\u0C7F]+',  # Telugu\n",
    "            r'[\\u0C80-\\u0CFF]+',  # Kannada\n",
    "            r'[\\u0D00-\\u0D7F]+',  # Malayalam\n",
    "            r'[\\u0D80-\\u0DFF]+',  # Sinhala\n",
    "            r'[\\u0E00-\\u0E7F]+',  # Thai\n",
    "            r'[\\u0E80-\\u0EFF]+',  # Lao\n",
    "            r'[\\u1000-\\u109F]+', # Myanmar\n",
    "            r'[\\u1780-\\u17FF]+', # Khmer\n",
    "            r'[\\u0600-\\u06FF]+', # Arabic\n",
    "            r'[\\u0590-\\u05FF]+', # Hebrew\n",
    "            r'[\\u0400-\\u04FF]+', # Cyrillic (Russian, etc.)\n",
    "            r'[\\u3040-\\u309F]+', # Hiragana (Japanese)\n",
    "            r'[\\u30A0-\\u30FF]+', # Katakana (Japanese)\n",
    "            r'[\\u4E00-\\u9FFF]+', # CJK Unified Ideographs (Chinese, Japanese, Korean)\n",
    "            r'[\\uAC00-\\uD7AF]+', # Hangul (Korean)\n",
    "            r'[\\u1100-\\u11FF]+', # Hangul Jamo (Korean)\n",
    "            r'[\\u0370-\\u03FF]+', # Greek\n",
    "            r'[\\u1F00-\\u1FFF]+', # Greek Extended\n",
    "            r'[\\u0100-\\u017F]+', # Latin Extended-A\n",
    "            r'[\\u0180-\\u024F]+', # Latin Extended-B\n",
    "            r'[\\u1E00-\\u1EFF]+', # Latin Extended Additional\n",
    "            r'[a-zA-Z]+',        # Basic Latin\n",
    "        ]\n",
    "\n",
    "        # Combine all Unicode patterns\n",
    "        multilingual_word_pattern = '|'.join(unicode_word_patterns)\n",
    "\n",
    "        # Combined pattern for tokenization\n",
    "        combined_pattern = f'({url_pattern}|{email_pattern}|{date_pattern}|{number_pattern}|{multilingual_word_pattern}|{punctuation_pattern})'\n",
    "\n",
    "        return re.findall(combined_pattern, sentence)\n",
    "    \n",
    "    return word_tokenizer\n",
    "\n",
    "# Create language-specific tokenizer\n",
    "word_tokenizer = create_language_aware_word_tokenizer(current_lang_info)\n",
    "\n",
    "print(f\"Created tokenizer for {current_lang_info['name']} ({current_lang_info['script']} script)\")\n",
    "print(f\"Unicode range: {current_lang_info['unicode_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67b32606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 1000 Marathi text samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 12728.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed for Marathi language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize the loaded data\n",
    "tokenized_data = []\n",
    "LIMIT = min(len(texts), 1000)  # Use actual loaded data length\n",
    "\n",
    "print(f\"Tokenizing {LIMIT} {current_lang_info['name']} text samples...\")\n",
    "\n",
    "for paragraph in tqdm(texts[:LIMIT]):\n",
    "    sentences = sentence_tokenizer(paragraph)\n",
    "    tokenized_paragraph = [word_tokenizer(sentence) for sentence in sentences]\n",
    "    tokenized_data.append(tokenized_paragraph)\n",
    "\n",
    "print(f\"Tokenization completed for {current_lang_info['name']} language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f90de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences saved to 'tokenized_marathi_sentences.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save tokenized data in sentence form\n",
    "sentence_data = []\n",
    "\n",
    "for para_idx, paragraph in enumerate(tokenized_data):\n",
    "    paragraph_sentences = []\n",
    "    for sentence in paragraph:\n",
    "        # Join tokens back into sentence\n",
    "        sentence_text = \" \".join(sentence)\n",
    "        paragraph_sentences.append(sentence_text)\n",
    "    sentence_data.append(paragraph_sentences)\n",
    "\n",
    "# Create filename based on current language\n",
    "filename = f\"tokenized_{current_lang_info['name'].lower()}_sentences.json\"\n",
    "\n",
    "# Save to JSON file\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Tokenized sentences saved to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bd7b71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 2117\n",
      "Total Words: 18911\n",
      "Total Characters: 108152\n",
      "Avg. Sentence Length (words): 8.93\n",
      "Avg. Word Length (chars): 5.72\n",
      "Type-Token Ratio (TTR): 0.4526\n"
     ]
    }
   ],
   "source": [
    "total_sentences = 0\n",
    "total_words = 0\n",
    "total_chars = 0\n",
    "all_words = []\n",
    "\n",
    "for para in tokenized_data:\n",
    "    total_sentences += len(para)\n",
    "    for sentence in para:\n",
    "        total_words += len(sentence)\n",
    "        total_chars += sum(len(word) for word in sentence)\n",
    "        all_words.extend(sentence)\n",
    "\n",
    "avg_sentence_length = total_words / total_sentences\n",
    "avg_word_length = total_chars / total_words\n",
    "ttr = len(set(all_words)) / len(all_words)\n",
    "\n",
    "print(\"Total Sentences:\", total_sentences)\n",
    "print(\"Total Words:\", total_words)\n",
    "print(\"Total Characters:\", total_chars)\n",
    "print(\"Avg. Sentence Length (words):\", round(avg_sentence_length, 2))\n",
    "print(\"Avg. Word Length (chars):\", round(avg_word_length, 2))\n",
    "print(\"Type-Token Ratio (TTR):\", round(ttr, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077c8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages:\n",
      "  hin_Deva - Hindi (Devanagari)\n",
      "  mar_Deva - Marathi (Devanagari)\n",
      "  guj_Gujr - Gujarati (Gujarati)\n",
      "  ben_Beng - Bengali (Bengali)\n",
      "  tam_Taml - Tamil (Tamil)\n",
      "  tel_Telu - Telugu (Telugu)\n",
      "  kan_Knda - Kannada (Kannada)\n",
      "  mal_Mlym - Malayalam (Malayalam)\n",
      "  pan_Guru - Punjabi (Gurmukhi)\n",
      "  ori_Orya - Odia (Odia)\n",
      "  asm_Beng - Assamese (Bengali)\n",
      "  urd_Arab - Urdu (Arabic)\n",
      "\n",
      "To process a new language, call: process_language('language_split', num_samples)\n"
     ]
    }
   ],
   "source": [
    "# ===== EASY LANGUAGE SWITCHING DEMO =====\n",
    "# To process a different language, simply change this variable and run all cells:\n",
    "\n",
    "def process_language(language_split, num_samples=100):\n",
    "    \"\"\"\n",
    "    Complete pipeline to process any language from IndicCorpV2\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROCESSING LANGUAGE: {language_split}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data\n",
    "    texts_new, lang_info_new = load_language_data(language_split, num_samples)\n",
    "    \n",
    "    if not texts_new:\n",
    "        print(\"Failed to load data. Skipping this language.\")\n",
    "        return\n",
    "    \n",
    "    # Create tokenizer\n",
    "    word_tokenizer_new = create_language_aware_word_tokenizer(lang_info_new)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_data_new = []\n",
    "    for paragraph in tqdm(texts_new[:num_samples], desc=f\"Tokenizing {lang_info_new['name']}\"):\n",
    "        sentences = sentence_tokenizer(paragraph)\n",
    "        tokenized_paragraph = [word_tokenizer_new(sentence) for sentence in sentences]\n",
    "        tokenized_data_new.append(tokenized_paragraph)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_sentences = sum(len(para) for para in tokenized_data_new)\n",
    "    total_words = sum(len(sentence) for para in tokenized_data_new for sentence in para)\n",
    "    total_chars = sum(len(word) for para in tokenized_data_new for sentence in para for word in sentence)\n",
    "    \n",
    "    print(f\"\\n{lang_info_new['name']} Language Statistics:\")\n",
    "    print(f\"- Total Sentences: {total_sentences}\")\n",
    "    print(f\"- Total Words: {total_words}\")\n",
    "    print(f\"- Total Characters: {total_chars}\")\n",
    "    print(f\"- Average Sentence Length: {total_words/total_sentences:.2f} words\")\n",
    "    print(f\"- Average Word Length: {total_chars/total_words:.2f} characters\")\n",
    "    \n",
    "    # Save tokenized data\n",
    "    filename_new = f\"tokenized_{lang_info_new['name'].lower()}_sentences.json\"\n",
    "    sentence_data_new = []\n",
    "    for paragraph in tokenized_data_new:\n",
    "        paragraph_sentences = [\" \".join(sentence) for sentence in paragraph]\n",
    "        sentence_data_new.append(paragraph_sentences)\n",
    "    \n",
    "    with open(filename_new, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sentence_data_new, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"- Saved to: {filename_new}\")\n",
    "    return tokenized_data_new, lang_info_new\n",
    "\n",
    "# Demo: Process multiple languages easily\n",
    "print(\"Available languages:\")\n",
    "for split, info in LANGUAGE_CONFIG.items():\n",
    "    print(f\"  {split} - {info['name']} ({info['script']})\")\n",
    "\n",
    "# Uncomment any of these to process different languages:\n",
    "# process_language(\"hin_Deva\", 50)  # Hindi\n",
    "# process_language(\"ben_Beng\", 50)  # Bengali  \n",
    "# process_language(\"tam_Taml\", 50)  # Tamil\n",
    "# process_language(\"tel_Telu\", 50)  # Telugu\n",
    "# process_language(\"guj_Gujr\", 50)  # Gujarati\n",
    "\n",
    "print(\"\\nTo process a new language, call: process_language('language_split', num_samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde42fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PROCESSING LANGUAGE: hin_Deva\n",
      "==================================================\n",
      "Loading Hindi data from split: hin_Deva\n",
      "Script: Devanagari\n",
      "Successfully loaded 25 text samples\n",
      "Sample 1: लोगों को बिलों संबंधी सुविधा देना ही उनका काम...\n",
      "Sample 2: ...\n",
      "Sample 3: इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्र...\n",
      "Successfully loaded 25 text samples\n",
      "Sample 1: लोगों को बिलों संबंधी सुविधा देना ही उनका काम...\n",
      "Sample 2: ...\n",
      "Sample 3: इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्र...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Hindi: 100%|██████████| 25/25 [00:00<00:00, 13347.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hindi Language Statistics:\n",
      "- Total Sentences: 43\n",
      "- Total Words: 662\n",
      "- Total Characters: 2714\n",
      "- Average Sentence Length: 15.40 words\n",
      "- Average Word Length: 4.10 characters\n",
      "- Saved to: tokenized_hindi_sentences.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hindi_data, hindi_info = process_language(\"hin_Deva\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978800ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
